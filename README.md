# ğŸ•·ï¸ Web Scraping Project â€” Selenium & Python

## ğŸ“Œ Overview

This project is a **web scraping application** built with **Python and Selenium** to extract structured data from a dynamically rendered website (JavaScript-based).

The scraper collects product information in an automated and reliable way, following best practices such as **rate limiting**, **error handling**, and **object-oriented design**.

---

## ğŸ› ï¸ Technologies Used

- Python 3.12
- Selenium WebDriver
- Chromium / ChromeDriver
- Pandas
- Pytest
- Docker & Docker Compose
- YAML configuration

---

## ğŸ“‚ Project Structure

```text
web_scraper/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scraper.py          # Selenium scraper logic
â”‚   â””â”€â”€ data_processor.py   # CSV / JSON data handling
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_scraper.py     # Unit tests
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml         # Scraper configuration
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ web_scraper.py          # Main execution script
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## âš™ï¸ How the Scraper Works

The project is structured around a single reusable class called **`WebScraper`**, responsible for:

- Initializing the browser  
- Loading the target website  
- Extracting product data  
- Handling missing elements safely  
- Applying rate limiting  
- Closing the browser correctly  

This design allows easy configuration and future scalability.

---

## ğŸ§  Main Features

- Headless browser execution  
- Dynamic content handling  
- Structured data extraction  
- Graceful error handling  
- Configurable timeout and rate limit  

---

## ğŸ§© Extracted Data

For each product, the scraper collects:

- Name  
- Price  
- Description  
- Rating (based on star count)  
- Reviews text  

The data is stored as:

CSV file with timestamp

JSON file with timestamp

---

## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Create and activate virtual environment

```text
python3 -m venv venv
source venv/bin/activate
```
### 2ï¸âƒ£ Install dependencies

```text
pip install -r requirements.txt
```

### 3ï¸âƒ£ Run the scraper

```text
python web_scraper.py
```

### ğŸ³ Run with Docker
Build and start container

```text
docker-compose up --build
```

The scraper will execute automatically and extract product data.

### ğŸ§ª Running Tests

```text
pytest
```

### âš ï¸ Error Handling & Rate Limiting

Missing elements are handled using NoSuchElementException

The scraper continues execution even if some product fields are not found

A configurable delay (rate_limit) prevents excessive requests

---

### ğŸ“ˆ Possible Enhancements

Export data to CSV or JSON

Pagination support

Database integration

Logging system

Proxy and user-agent rotation
